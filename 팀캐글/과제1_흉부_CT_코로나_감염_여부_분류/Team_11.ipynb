{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c50f01a-72fd-4804-9f54-d77b52d3991f",
   "metadata": {},
   "source": [
    "# [team-11] 흉부 CT 코로나 감염 여부 분류\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51ba7c1-0393-47ec-89d9-f6f97072773b",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a45c7e-10ca-4fd1-9fd4-6326313a631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, copy, cv2, sys, random\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# from datetime import datetime, timezone, timedelta\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pdb\n",
    "import wandb\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import optuna\n",
    "\n",
    "from pylab import rcParams\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "cv2.setNumThreads(0)\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "\n",
    "import timm\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6c255b-b30d-4ffd-a663-bc01a2c37954",
   "metadata": {},
   "source": [
    "## Set Arguments & hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9c4250-2257-404f-941d-58eff1e9eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드(seed) 설정\n",
    "\n",
    "RANDOM_SEED = 2022\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d69a8bc-2e64-4de6-928f-4e16957f6af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "### 데이터 디렉토리 설정 ###\n",
    "DATA_DIR= '/USER/Comp2/data'\n",
    "NUM_CLS = 2\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.0005\n",
    "EARLY_STOPPING_PATIENCE = 3\n",
    "INPUT_SHAPE = 256\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44807b0-7788-49ec-aff2-c756e4513c5e",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243d2bcd-1dab-4e5e-bbb4-6bb53e880524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(dataset):\n",
    "  meanRGB = [np.mean(image.numpy(), axis=(1,2)) for image,_ in dataset]\n",
    "  stdRGB = [np.std(image.numpy(), axis=(1,2)) for image,_ in dataset]\n",
    "\n",
    "  meanR = np.mean([m[0] for m in meanRGB])\n",
    "  meanG = np.mean([m[1] for m in meanRGB])\n",
    "  meanB = np.mean([m[2] for m in meanRGB])\n",
    "\n",
    "  stdR = np.mean([s[0] for s in stdRGB])\n",
    "  stdG = np.mean([s[1] for s in stdRGB])\n",
    "  stdB = np.mean([s[2] for s in stdRGB])\n",
    "\n",
    "  print(meanR, meanG, meanB)\n",
    "  print(stdR, stdG, stdB)\n",
    "\n",
    "trans = transforms.Compose([\n",
    "                            transforms.Resize([INPUT_SHAPE,INPUT_SHAPE]),\n",
    "                            transforms.ToTensor()\n",
    "])\n",
    "temp_imgfd = ImageFolder(DATA_DIR,transform=trans)\n",
    "len(temp_imgfd)\n",
    "\n",
    "get_mean_std(temp_imgfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092461b2-0ebb-4c9e-b560-c8b875ede77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=[0.5952265 ,0.59483945 ,0.5946652]\n",
    "std=[0.29298505, 0.29308468 ,0.2930554]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b81fa5-3756-46aa-b3cb-6f19879aba05",
   "metadata": {},
   "source": [
    "#### Train & Validation Set loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1026861-c90d-40cd-a30c-30c3709d1952",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AugMix(ImageOnlyTransform):\n",
    "    \"\"\"Augmentations mix to Improve Robustness and Uncertainty.\n",
    "    Args:\n",
    "        image (np.ndarray): Raw input image of shape (h, w, c)\n",
    "        severity (int): Severity of underlying augmentation operators.\n",
    "        width (int): Width of augmentation chain\n",
    "        depth (int): Depth of augmentation chain. -1 enables stochastic depth uniformly\n",
    "          from [1, 3]\n",
    "        alpha (float): Probability coefficient for Beta and Dirichlet distributions.\n",
    "        augmentations (list of augmentations): Augmentations that need to mix and perform.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, width=2, depth=2, alpha=0.5, augmentations=[A.HorizontalFlip()], always_apply=False, p=0.5):\n",
    "        super(AugMix, self).__init__(always_apply, p)\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.alpha = alpha\n",
    "        self.augmentations = augmentations\n",
    "        self.ws = np.float32(np.random.dirichlet([self.alpha] * self.width))\n",
    "        self.m = np.float32(np.random.beta(self.alpha, self.alpha))\n",
    "\n",
    "    def apply_op(self, image, op):\n",
    "        image = op(image=image)[\"image\"]\n",
    "        return image\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "        mix = np.zeros_like(img)\n",
    "        for i in range(self.width):\n",
    "            image_aug = img.copy()\n",
    "\n",
    "            for _ in range(self.depth):\n",
    "                op = np.random.choice(self.augmentations)\n",
    "                image_aug = self.apply_op(image_aug, op)\n",
    "\n",
    "            mix = np.add(mix, self.ws[i] * image_aug, out=mix, casting=\"unsafe\")\n",
    "\n",
    "        mixed = (1 - self.m) * img + self.m * mix\n",
    "        if img.dtype in [\"uint8\", \"uint16\", \"uint32\", \"uint64\"]:\n",
    "            mixed = np.clip((mixed), 0, 255).astype(np.uint8)\n",
    "        return mixed\n",
    "\n",
    "    def get_transform_init_args_names(self):\n",
    "        return (\"width\", \"depth\", \"alpha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74355af7-0503-4c26-baab-b54dc1a68e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "augs = [\n",
    "        A.HorizontalFlip(always_apply=True),\n",
    "        A.Blur(always_apply=True),\n",
    "        A.OneOf(\n",
    "                [A.ShiftScaleRotate(always_apply=True),\n",
    "                A.GaussNoise(always_apply=True)]\n",
    "                ),\n",
    "        A.Cutout(always_apply=True),\n",
    "        A.CLAHE(always_apply=True),\n",
    "       ]\n",
    "\n",
    "\n",
    "augmix = A.Compose([\n",
    "    AugMix(width=3, depth=3, alpha=0.5, augmentations=augs, always_apply=False, p=0.5),\n",
    "    A.Normalize(mean, std),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04642777-c2e0-439b-9692-f6c571a86521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, mode, input_shape,transform):\n",
    "        self.data_dir = data_dir\n",
    "        self.mode = mode\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Loading dataset\n",
    "        self.db = self.data_loader()\n",
    "        \n",
    "        # Dataset split\n",
    "        if self.mode == 'train':\n",
    "            self.db = self.db[:int(len(self.db) * 0.9)]\n",
    "        elif self.mode == 'val':\n",
    "            self.db = self.db[int(len(self.db) * 0.9):]\n",
    "            self.db.reset_index(inplace=True)\n",
    "        else:\n",
    "            print(f'!!! Invalid split {self.mode}... !!!')\n",
    "            \n",
    "        # Transform function\n",
    "        self.transform = transform\n",
    "\n",
    "    def data_loader(self):\n",
    "        print('Loading ' + self.mode + ' dataset..')\n",
    "        if not os.path.isdir(self.data_dir):\n",
    "            print(f'!!! Cannot find {self.data_dir}... !!!')\n",
    "            sys.exit()\n",
    "        \n",
    "        # (COVID : 1, No : 0)\n",
    "        db = pd.read_csv(os.path.join(self.data_dir, 'train.csv'))\n",
    "        \n",
    "        return db\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = copy.deepcopy(self.db.loc[index])\n",
    "\n",
    "        # Loading image\n",
    "        cvimg = cv2.imread(os.path.join(self.data_dir,'train',data['file_name']),\n",
    "                           cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION\n",
    "                          )\n",
    "        if not isinstance(cvimg, np.ndarray):\n",
    "            raise IOError(\"Fail to read %s\" % data['file_name'])\n",
    "\n",
    "        cvimg = cv2.cvtColor(cvimg,cv2.COLOR_BGR2RGB)\n",
    "        cvimg = cv2.resize(cvimg, (self.input_shape, self.input_shape))\n",
    "        \n",
    "        # Preprocessing images\n",
    "        trans_image = self.transform(image=cvimg)['image']\n",
    "\n",
    "        return trans_image, data['COVID']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b27520-c82c-4ec8-ae0b-119a79167f09",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca31685e-17ee-4183-a4fa-1888d2016a94",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cea68f0-dfad-47ce-a8ca-00ea01988886",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load dataset & dataloader\n",
    "train_dataset = CustomDataset(data_dir=DATA_DIR, mode='train', input_shape=INPUT_SHAPE,transform=augmix)\n",
    "validation_dataset = CustomDataset(data_dir=DATA_DIR, mode='val', input_shape=INPUT_SHAPE,transform=augmix)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print('Train set samples:',len(train_dataset),  'Val set samples:', len(validation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995d1a03-0b83-48cf-a4f9-16a8fec6fef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_imgs(dataset):\n",
    "    rcParams['figure.figsize'] = 20,10\n",
    "    for i in range(2):\n",
    "        f, axarr = plt.subplots(1,5)\n",
    "        for p in range(5):\n",
    "            idx = np.random.randint(0, len(dataset))\n",
    "            img, label = dataset[idx]\n",
    "            axarr[p].imshow(img.permute(1,2,0))\n",
    "            axarr[p].set_title(f'{idx}, covid : {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53a5f7b-7afd-4be3-a543-9a7b6ff53ab0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_imgs(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150c47b5-4646-4d55-90ea-4817f054eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_dataset[1][0] \n",
    "print(sample.dtype, sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e0b73-f323-40ea-b372-6c1d607618a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class custom_CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(custom_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=25, kernel_size=5)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=25*61*61, out_features=32)\n",
    "        self.fc2 = nn.Linear(in_features=32, out_features=num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x))) # -4 //2\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        x = torch.flatten(x,1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        output = self.softmax(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d056905-1f77-4579-a260-07bb1056f6db",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utils\n",
    "### EarlyStopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c3315-ebca-4e6b-a8f2-1281ccd0bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossEarlyStopper():\n",
    "    \"\"\"Early stopper\n",
    "    \n",
    "    Attributes:\n",
    "        patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n",
    "        patience_counter (int): loss 가 줄어들지 않을 때 마다 1씩 증가, 감소 시 0으로 리셋\n",
    "        min_loss (float): 최소 loss\n",
    "        stop (bool): True 일 때 학습 중단\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience: int)-> None:\n",
    "        self.patience = patience\n",
    "\n",
    "        self.patience_counter = 0\n",
    "        self.min_loss = np.Inf\n",
    "        self.stop = False\n",
    "        self.save_model = False\n",
    "\n",
    "    def check_early_stopping(self, loss: float)-> None:\n",
    "        \"\"\"Early stopping 여부 판단\"\"\"  \n",
    "        self.save_model=False\n",
    "        \n",
    "        if self.min_loss == np.Inf:\n",
    "            self.min_loss = loss\n",
    "            return None\n",
    "\n",
    "        elif loss > self.min_loss:\n",
    "            self.patience_counter += 1\n",
    "            msg = f\"Early stopping counter {self.patience_counter}/{self.patience}\"\n",
    "\n",
    "            if self.patience_counter == self.patience:\n",
    "                self.stop = True\n",
    "                \n",
    "        elif loss <= self.min_loss:\n",
    "            self.patience_counter = 0\n",
    "            self.save_model = True\n",
    "            msg = f\"Validation loss decreased {self.min_loss:.3f} -> {loss:.3f}\"\n",
    "            self.min_loss = loss\n",
    "        \n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaffd8d-b025-42c1-8dd8-69529487389e",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faaac1b-64c3-4659-82de-d4309502f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    \"\"\" epoch에 대한 학습 및 검증 절차 정의\"\"\"\n",
    "    \n",
    "    def __init__(self, loss_fn, model, device, metric_fn, optimizer=None, scheduler=None):\n",
    "        \"\"\" 초기화\n",
    "        \"\"\"\n",
    "        self.loss_fn = loss_fn\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.metric_fn = metric_fn\n",
    "\n",
    "    def train_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 학습 절차\"\"\"\n",
    "        \n",
    "        self.model.train()\n",
    "        train_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).float()\n",
    "            \n",
    "            pred = self.model(img)\n",
    "            \n",
    "            loss = self.loss_fn(pred[:,1], label)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            train_total_loss += loss.item()\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "        self.train_mean_loss = train_total_loss / batch_index\n",
    "        self.train_score, f1 = self.metric_fn(y_pred=pred_lst, y_answer=target_lst)\n",
    "        msg = f'Epoch {epoch_index}, Train loss: {self.train_mean_loss:.3f}, Acc: {self.train_score:.3f}, F1-Macro: {f1:.3f}'\n",
    "        print(msg)\n",
    "\n",
    "    def validate_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 검증 절차\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        val_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).float()\n",
    "            pred = self.model(img)\n",
    "            \n",
    "            loss = self.loss_fn(pred[:,1], label)\n",
    "            val_total_loss += loss.item()\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "        self.val_mean_loss = val_total_loss / batch_index\n",
    "        self.validation_score, f1 = self.metric_fn(y_pred=pred_lst, y_answer=target_lst)\n",
    "        msg = f'Epoch {epoch_index}, Val loss: {self.val_mean_loss:.3f}, Acc: {self.validation_score:.3f}, F1-Macro: {f1:.3f}'\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aca506-d168-4c9f-8eca-5cdecb122961",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33678d90-a254-48d5-bf09-2a817eeafea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def get_metric_fn(y_pred, y_answer):\n",
    "    \"\"\" 성능을 반환하는 함수\"\"\"\n",
    "    \n",
    "    assert len(y_pred) == len(y_answer), 'The size of prediction and answer are not same.'\n",
    "    accuracy = accuracy_score(y_answer, y_pred)\n",
    "    f1 = f1_score(y_answer, y_pred, average='macro')\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d729c079-9d85-49ce-857f-320b0c56a3a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train\n",
    "### 학습을 위한 객체 선언"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb8dae0-8e32-4ac0-a585-858a7095d2a4",
   "metadata": {},
   "source": [
    "#### Load model and other utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d52e1-752a-40d5-9b34-c06d3dbdd45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = custom_CNN(NUM_CLS).to(DEVICE)\n",
    "\n",
    "# # Save Initial Model\n",
    "# torch.save(model.state_dict(), 'initial.pt')\n",
    "\n",
    "# Set optimizer, scheduler, loss function, metric function\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler =  optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e5, max_lr=0.0001, epochs=EPOCHS, steps_per_epoch=len(train_dataloader))\n",
    "loss_fn = nn.BCELoss()\n",
    "metric_fn = get_metric_fn\n",
    "\n",
    "\n",
    "# Set trainer\n",
    "trainer = Trainer(loss_fn, model, DEVICE, metric_fn, optimizer, scheduler)\n",
    "\n",
    "# Set earlystopper\n",
    "early_stopper = LossEarlyStopper(patience=EARLY_STOPPING_PATIENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aa8aef-b984-4133-b6b2-e1c85900f724",
   "metadata": {},
   "source": [
    "### epoch 단위 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc35f70-25fc-48e1-92f8-8633b3b8be80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch_index in tqdm(range(EPOCHS)):\n",
    "\n",
    "    trainer.train_epoch(train_dataloader, epoch_index)\n",
    "    trainer.validate_epoch(validation_dataloader, epoch_index)\n",
    "\n",
    "    # early_stopping check\n",
    "    early_stopper.check_early_stopping(loss=trainer.val_mean_loss)\n",
    "\n",
    "    if early_stopper.stop:\n",
    "        print('Early stopped')\n",
    "        break\n",
    "\n",
    "    if early_stopper.save_model:\n",
    "        check_point = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict()\n",
    "        }\n",
    "        torch.save(check_point, 'best.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81f4b29-93f0-442f-ba63-352f512b1cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPUtil import showUtilization as gpu_usage\n",
    "gpu_usage()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe53514a-e83f-4795-9589-640f26cc2993",
   "metadata": {},
   "source": [
    "## Inference\n",
    "### 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6729cfde-c4b3-4d36-938e-f8bb8d8afef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINED_MODEL_PATH = 'best.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bbba92-b53c-499f-b5f9-b6ac3edde331",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced90de9-50ec-4e18-9f42-5a1b493941a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data_dir, input_shape,transform):\n",
    "        self.data_dir = data_dir\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Loading dataset\n",
    "        self.db = self.data_loader()\n",
    "        \n",
    "        # Transform function\n",
    "        self.transform = transform\n",
    "\n",
    "    def data_loader(self):\n",
    "        print('Loading test dataset..')\n",
    "        if not os.path.isdir(self.data_dir):\n",
    "            print(f'!!! Cannot find {self.data_dir}... !!!')\n",
    "            sys.exit()\n",
    "        \n",
    "        db = pd.read_csv(os.path.join(self.data_dir, 'sample_submission.csv'))\n",
    "        return db\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = copy.deepcopy(self.db.loc[index])\n",
    "        \n",
    "        # Loading image\n",
    "        cvimg = cv2.imread(os.path.join(self.data_dir,'test',data['file_name']),\n",
    "                           # 0\n",
    "                           cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION\n",
    "                          )\n",
    "        if not isinstance(cvimg, np.ndarray):\n",
    "            raise IOError(\"Fail to read %s\" % data['file_name'])\n",
    "\n",
    "        cvimg = cv2.cvtColor(cvimg,cv2.COLOR_BGR2RGB)\n",
    "        cvimg = cv2.resize(cvimg, (self.input_shape, self.input_shape))\n",
    "        \n",
    "        # Preprocessing images\n",
    "        trans_image = self.transform(image = cvimg)['image']\n",
    "\n",
    "        return trans_image, data['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb6e79d-c29e-4cee-a18c-2ff798a5b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A_test = A.Compose([\n",
    "    A.Normalize(mean,std),\n",
    "    ToTensorV2()\n",
    "])\n",
    "test_dataset = TestDataset(data_dir=DATA_DIR, input_shape=INPUT_SHAPE,transform=A_test)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd31a3d-08cd-48fc-87b0-137976d4d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset & dataloader\n",
    "test_dataset = TestDataset(data_dir=DATA_DIR, input_shape=INPUT_SHAPE,transform=A_test)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53efd72b-172d-4e34-a1dd-65ed8c745b58",
   "metadata": {},
   "source": [
    "### 추론 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a090ea-bb34-4d3d-a127-b1190e8c416c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('best.pt')['model'])\n",
    "model.load_state_dict(torch.load(TRAINED_MODEL_PATH)['model'])\n",
    "\n",
    "\n",
    "# Prediction\n",
    "file_lst = []\n",
    "pred_lst = []\n",
    "prob_lst = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_index, (img, file_num) in tqdm(enumerate(test_dataloader)):\n",
    "        img = img.to(DEVICE)\n",
    "        pred = model(img)\n",
    "        file_lst.extend(list(file_num))\n",
    "        pred_lst.extend(pred.argmax(dim=1).tolist())\n",
    "        prob_lst2.extend(pred[:, 1].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056169d1-64a8-4b81-8daf-722b029cf2b9",
   "metadata": {},
   "source": [
    "### 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f133cd86-b87b-4f8b-ae0e-c240655ae9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'file_name':file_lst, 'COVID':pred_lst})\n",
    "# df.sort_values(by=['file_name'], inplace=True)\n",
    "df.to_csv('final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba390903-7101-468a-b991-a6d30b60f36e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
