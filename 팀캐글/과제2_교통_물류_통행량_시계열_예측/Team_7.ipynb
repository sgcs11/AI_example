{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-B5aZDd0qVs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import math\n",
        "import wandb\n",
        "import time\n",
        "\n",
        "from glob import glob\n",
        "from copy import deepcopy\n",
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.modules.loss import _Loss\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GJFOXQEmjd_"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM approach"
      ],
      "metadata": {
        "id": "dwzbPzENt6pw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "laHSp_KSE_gc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMu_jlgNmjeB"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        src, trg = self.data[idx]\n",
        "        (last_2week, last_week) = src\n",
        "        src_ = np.concatenate((last_2week[np.newaxis,:], last_week[np.newaxis,:]), axis=0) # 2*168\n",
        "        src = torch.Tensor(src_)\n",
        "        trg = torch.Tensor(trg)\n",
        "        return src, trg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(data, number):\n",
        "    source = []\n",
        "    target = []\n",
        "\n",
        "    for i in range(4, len(data)//(24*7)-2):\n",
        "        last_2week = data[5*24 + (i)*7*24:5*24 + (i+1)*7*24, number+2]\n",
        "        last_week = data[5*24 + (i+1)*7*24:5*24 + (i+2)*7*24, number+2]\n",
        "        trg = data[5*24 + (i+2)*7*24:5*24 + (i+3)*7*24, number+2]\n",
        "        source.append([last_2week, last_week]) # 5*24:5*24+7*24\n",
        "        target.append(trg)\n",
        "    print(len(source), len(target))\n",
        "    train_data = list(zip(source[:int(len(source)*0.8)], target[:int(len(source)*0.8)]))\n",
        "    valid_data = list(zip(source[int(len(source)*0.8):], target[int(len(source)*0.8):]))\n",
        "    return train_data, valid_data"
      ],
      "metadata": {
        "id": "lL9OePAnmr-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataframe(df):\n",
        "    df = df.copy()\n",
        "    head = df.iloc[:1416,:]\n",
        "    tmp_0222 = df.iloc[1416:1429,:]\n",
        "    division = df[df['날짜']==20200229].iloc[:13,2:].sum().mean() / \\\n",
        "        ((df[df['날짜']==20200222].iloc[:13,2:].sum().mean() + df[df['날짜']==20200307].iloc[:13,2:].sum().mean())/2)\n",
        "    tmp_0222_ = (df[df['날짜']==20200222].iloc[13:,2:].reset_index(drop=True) + \\\n",
        "                 df[df['날짜']==20200307].iloc[13:,2:].reset_index(drop=True))/2*division\n",
        "    tmp_0222__ = pd.concat((tmp_0222.iloc[:11,:2].reset_index(drop=True), tmp_0222_),axis=1)\n",
        "    for_0222 = pd.concat((tmp_0222, tmp_0222__), axis=0)\n",
        "    middle = df.iloc[1429:2125,:]\n",
        "    tmp_0330 = pd.concat((df.iloc[:2,:2].reset_index(drop=True), df.iloc[2125:2127].iloc[:2,2:].reset_index(drop=True) * 2), axis=1)\n",
        "    division = tmp_0330.iloc[:2,2:].sum().mean() / \\\n",
        "        ((df[df['날짜']==20200406].iloc[:2,2:].reset_index(drop=True).sum().mean()+df[df['날짜']==20200323].iloc[:2,2:].reset_index(drop=True).sum().mean())/2)\n",
        "    tmp_0330_ = pd.concat((df.iloc[2125:2127,:2].reset_index(drop=True), ((df[df['날짜']==20200406].iloc[2:,2:].reset_index(drop=True)+ \\\n",
        "                df[df['날짜']==20200323].iloc[2:,2:].reset_index(drop=True))/2)*division), axis=1)\n",
        "    tmp_0330_['날짜'] = 20200330.0\n",
        "    for_0330 = pd.concat((tmp_0330.reset_index(drop=True), tmp_0330_.reset_index(drop=True)), axis=0)\n",
        "    tail = df.iloc[2127:,:]\n",
        "    pre_df = pd.concat((head, for_0222, middle, for_0330, tail), axis=0).reset_index(drop=True)\n",
        "    return pre_df"
      ],
      "metadata": {
        "id": "zh35bhUor_vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "z8qKDv0YmtEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Swish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.sigmoid(x)"
      ],
      "metadata": {
        "id": "BVzmLT1wR5a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size=168,      # input 길이는 168시간(7일 X 24시간)\n",
        "                 hidden_size=1024,\n",
        "                 output_size=168,      # output 길이는 168시간(7일 X 24시간)\n",
        "                 num_layers=2,\n",
        "                 dropout=0,\n",
        "                 batch_first=True): \n",
        "        super(MyModel, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm1 = nn.LSTM(input_size,\n",
        "                             hidden_size,\n",
        "                             dropout=dropout,\n",
        "                             num_layers=num_layers,\n",
        "                             batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(hidden_size, \n",
        "                             hidden_size,\n",
        "                             dropout=dropout,\n",
        "                             num_layers=num_layers,\n",
        "                             batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size*2, \n",
        "                                output_size)\n",
        "        # self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.activation = Swish()\n",
        "\n",
        "\n",
        "    def forward(self, x, h_in, c_in):\n",
        "        h_in = nn.Parameter(h_in.float(), requires_grad=True)      # gradient descent로 업데이트 되는(requires_grad=True), hidden state 초기값 파라미터 생성 \n",
        "        c_in = nn.Parameter(c_in.float(), requires_grad=True)      # gradient descent로 업데이트 되는(requires_grad=True), cell state 초기값 파라미터 생성\n",
        "\n",
        "        # Layer 1\n",
        "        lstm_out, (h_1, c_1) = self.lstm1(x, (h_in, c_in))\n",
        "        lstm_out = self.activation(lstm_out)\n",
        "\n",
        "        # Layer2\n",
        "        lstm_out, (h_2, c_2) = self.lstm2(lstm_out, (h_1, c_1))\n",
        "        lstm_out = self.activation(lstm_out) # L, B, hidden = 2, 4, 1024 -> 1, 4, 2048\n",
        "\n",
        "        # Final\n",
        "        lstm_out_ = torch.cat((lstm_out[:,0,:], lstm_out[:,1,:]), dim=-1)\n",
        "        out = self.linear(lstm_out_)\n",
        "        # out = self.out(self.activation(out))\n",
        "        \n",
        "        return out, (h_1, c_1) # (h_2, c_2)"
      ],
      "metadata": {
        "id": "QUkfAG5Gmv5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization"
      ],
      "metadata": {
        "id": "FC7UZYpem1HK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weight(model, kind='xavier'):\n",
        "    for name, i in model.named_parameters():\n",
        "        if kind == 'xavier':\n",
        "            if i.dim() < 2:\n",
        "                continue\n",
        "            if 'weight' in name:\n",
        "                init.xavier_normal_(i, gain=1.0)\n",
        "            elif 'bias' in name:\n",
        "                init.xavier_uniform_(i, gain=1.0)\n",
        "            else:\n",
        "                pass\n",
        "        elif kind == 'kaiming':\n",
        "            if i.dim() < 2:\n",
        "                continue\n",
        "            if 'weight' in name:\n",
        "                init.kaiming_normal_(i)\n",
        "            elif 'bias' in name:\n",
        "                init.kaiming_uniform_(i)\n",
        "            else:\n",
        "                pass"
      ],
      "metadata": {
        "id": "__Y9hMK7m2Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss"
      ],
      "metadata": {
        "id": "YqKMy-tam3JT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyLoss(_Loss):\n",
        "    def __init__(self): # RMSE\n",
        "        super(MyLoss, self).__init__()\n",
        "        self.lossMSE = nn.MSELoss() \n",
        "        \n",
        "    def forward(self, preds, trg):\n",
        "        return torch.sqrt(self.lossMSE(preds, trg))"
      ],
      "metadata": {
        "id": "6nSSayufm36Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metric"
      ],
      "metadata": {
        "id": "znTu4TX5S5qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_error(preds, trg):\n",
        "    summation = ((preds - trg) ** 2).mean() # 1*168\n",
        "    return summation"
      ],
      "metadata": {
        "id": "B5iMMWeDS68K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Functions"
      ],
      "metadata": {
        "id": "Sp0zlWIPm6rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, criterion, train_loader, optimizer, scheduler, device, mini, maxi):\n",
        "    model.train()\n",
        "    \n",
        "    losses = 0\n",
        "    total_mse = 0\n",
        "    for idx, (src, trg) in tqdm(enumerate(train_loader)):\n",
        "        src, trg = src.to(device).float(), trg.to(device).float()\n",
        "\n",
        "        (h_0, c_0) = (torch.zeros(model.lstm1.num_layers, src.size(0), model.hidden_size, requires_grad=True).to(device),\n",
        "                     torch.zeros(model.lstm1.num_layers, src.size(0), model.hidden_size, requires_grad=True).to(device))\n",
        "        \n",
        "        outs, (h_in, c_in) = model(src, h_0, c_0)\n",
        "\n",
        "        loss = criterion(outs, trg)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        losses += loss.item()\n",
        "\n",
        "        outs, trg = (outs.detach().cpu().numpy()*(maxi-mini) + mini), \\\n",
        "                    (trg.detach().cpu().numpy()*(maxi-mini) + mini)\n",
        "\n",
        "        mse = mse_error(outs, trg)\n",
        "        total_mse += mse\n",
        "\n",
        "    total_mse = total_mse\n",
        "\n",
        "    return losses, total_mse\n",
        "        \n",
        "def valid_one_epoch(model, criterion, valid_loader, device, mini, maxi):\n",
        "    model.eval()\n",
        "    print(\"validation\")\n",
        "    \n",
        "    losses = 0\n",
        "    total_mse = 0\n",
        "    with torch.no_grad():\n",
        "        for idx, (src, trg) in tqdm(enumerate(valid_loader)):\n",
        "            src, trg = src.to(device).float(), trg.to(device).float()\n",
        "\n",
        "            (h_0, c_0) = (torch.zeros(model.lstm1.num_layers, src.size(0), model.hidden_size, requires_grad=False).to(device),\n",
        "                        torch.zeros(model.lstm1.num_layers, src.size(0), model.hidden_size, requires_grad=False).to(device))\n",
        "            \n",
        "            outs, (h_in, c_in) = model(src, h_0, c_0)\n",
        "\n",
        "            loss = criterion(outs, trg)\n",
        "            \n",
        "            losses += loss.item()\n",
        "            outs, trg = (outs.detach().cpu().numpy()*(maxi-mini) + mini), \\\n",
        "                        (trg.detach().cpu().numpy()*(maxi-mini) + mini)\n",
        "            mse = mse_error(outs, trg)\n",
        "            total_mse += mse            \n",
        "\n",
        "    total_mse = total_mse\n",
        "    \n",
        "    return losses, total_mse"
      ],
      "metadata": {
        "id": "VFHiO8zNm7dX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Early Stop"
      ],
      "metadata": {
        "id": "uogp28gTm-C-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopper():\n",
        "    def __init__(self, patience: int)-> None:\n",
        "        self.patience = patience\n",
        "\n",
        "        self.patience_counter = 0\n",
        "        self.best_mse = 1e9\n",
        "        self.stop = False\n",
        "        self.save_model = False\n",
        "\n",
        "    def check_early_stopping(self, score: float)-> None:\n",
        "        if self.best_mse == 1e9:\n",
        "            self.best_mse = score\n",
        "            return None\n",
        "\n",
        "        elif score >= self.best_mse:\n",
        "            self.patience_counter += 1\n",
        "            self.save_model = False\n",
        "            if self.patience_counter == self.patience:\n",
        "                self.stop = True\n",
        "                \n",
        "        elif score < self.best_mse:\n",
        "            self.patience_counter = 0\n",
        "            self.save_model = True\n",
        "            self.best_mse = score\n",
        "        print(\"best_mse\", self.best_mse, \"best_rmse\", np.sqrt(self.best_mse))"
      ],
      "metadata": {
        "id": "A1q9e2nGm_JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scheduler"
      ],
      "metadata": {
        "id": "zKUzpNyFnC4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CosineAnnealingWarmupRestarts(_LRScheduler):\n",
        "    \"\"\"\n",
        "        optimizer (Optimizer): Wrapped optimizer.\n",
        "        first_cycle_steps (int): First cycle step size.\n",
        "        cycle_mult(float): Cycle steps magnification. Default: -1.\n",
        "        max_lr(float): First cycle's max learning rate. Default: 0.1.\n",
        "        min_lr(float): Min learning rate. Default: 0.001.\n",
        "        warmup_steps(int): Linear warmup step size. Default: 0.\n",
        "        gamma(float): Decrease rate of max learning rate by cycle. Default: 1.\n",
        "        last_epoch (int): The index of last epoch. Default: -1.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 optimizer: torch.optim.Optimizer,\n",
        "                 first_cycle_steps: int,\n",
        "                 cycle_mult: float = 1.,\n",
        "                 max_lr: float = 0.1,\n",
        "                 min_lr: float = 0.001,\n",
        "                 warmup_steps: int = 0,\n",
        "                 gamma: float = 1.,\n",
        "                 last_epoch: int = -1\n",
        "                 ):\n",
        "        assert warmup_steps < first_cycle_steps\n",
        "\n",
        "        self.first_cycle_steps = first_cycle_steps  # first cycle step size\n",
        "        self.cycle_mult = cycle_mult  # cycle steps magnification\n",
        "        self.base_max_lr = max_lr  # first max learning rate\n",
        "        self.max_lr = max_lr  # max learning rate in the current cycle\n",
        "        self.min_lr = min_lr  # min learning rate\n",
        "        self.warmup_steps = warmup_steps  # warmup step size\n",
        "        self.gamma = gamma  # decrease rate of max learning rate by cycle\n",
        "\n",
        "        self.cur_cycle_steps = first_cycle_steps  # first cycle step size\n",
        "        self.cycle = 0  # cycle count\n",
        "        self.step_in_cycle = last_epoch  # step size of the current cycle\n",
        "\n",
        "        super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "        self.init_lr()\n",
        "\n",
        "    def init_lr(self):\n",
        "        self.base_lrs = []\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = self.min_lr\n",
        "            self.base_lrs.append(self.min_lr)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self.step_in_cycle == -1:\n",
        "            return self.base_lrs\n",
        "        elif self.step_in_cycle < self.warmup_steps:\n",
        "            return [(self.max_lr - base_lr) * self.step_in_cycle / self.warmup_steps + base_lr for base_lr in\n",
        "                    self.base_lrs]\n",
        "        else:\n",
        "            return [base_lr + (self.max_lr - base_lr) \\\n",
        "                    * (1 + math.cos(math.pi * (self.step_in_cycle - self.warmup_steps) \\\n",
        "                                    / (self.cur_cycle_steps - self.warmup_steps))) / 2\n",
        "                    for base_lr in self.base_lrs]\n",
        "\n",
        "    def step(self, epoch=None):\n",
        "        if epoch is None:\n",
        "            epoch = self.last_epoch + 1\n",
        "            self.step_in_cycle = self.step_in_cycle + 1\n",
        "            if self.step_in_cycle >= self.cur_cycle_steps:\n",
        "                self.cycle += 1\n",
        "                self.step_in_cycle = self.step_in_cycle - self.cur_cycle_steps\n",
        "                self.cur_cycle_steps = int(\n",
        "                    (self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\n",
        "        else:\n",
        "            if epoch >= self.first_cycle_steps:\n",
        "                if self.cycle_mult == 1.:\n",
        "                    self.step_in_cycle = epoch % self.first_cycle_steps\n",
        "                    self.cycle = epoch // self.first_cycle_steps\n",
        "                else:\n",
        "                    n = int(math.log((epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1), self.cycle_mult))\n",
        "                    self.cycle = n\n",
        "                    self.step_in_cycle = epoch - int(\n",
        "                        self.first_cycle_steps * (self.cycle_mult ** n - 1) / (self.cycle_mult - 1))\n",
        "                    self.cur_cycle_steps = self.first_cycle_steps * self.cycle_mult ** (n)\n",
        "            else:\n",
        "                self.cur_cycle_steps = self.first_cycle_steps\n",
        "                self.step_in_cycle = epoch\n",
        "\n",
        "        self.max_lr = self.base_max_lr * (self.gamma ** self.cycle)\n",
        "        self.last_epoch = math.floor(epoch)\n",
        "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
        "            param_group['lr'] = lr"
      ],
      "metadata": {
        "id": "sp1Uh3q9nEQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "36rrEr4ZnGBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "id": "Od5tzqufnHEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setting(number):\n",
        "    CFG = {\n",
        "        'model_name':'lstm',\n",
        "        'seed':2022,\n",
        "        'batch_size': 4,\n",
        "        'epoch': 100,\n",
        "        'initialization': \"kaiming\", # kaiming, xavier\n",
        "        'input_size':168,\n",
        "        'hidden_size':1024,\n",
        "        'output_size':168,\n",
        "        'num_layers':2,\n",
        "        'road_number':number,\n",
        "        'save_dir':'lstm_2week_minmax_swish',\n",
        "        'dropout':0.\n",
        "    }\n",
        "\n",
        "    root_dir = '/content/drive/Othercomputers/내 컴퓨터/workspace/ml_cartraffic'\n",
        "\n",
        "    wandb.config = CFG\n",
        "    experiment_name = 'lstm_minmax_2layer' # BrightContrast Vertical\n",
        "    run = wandb.init(project=f\"{CFG['model_name']}\", settings=wandb.Settings(start_method=\"thread\"), name=f\"{experiment_name}_{CFG['road_number']}\")\n",
        "\n",
        "    seed_everything(CFG['seed'])\n",
        "    # device 설정\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    # Data 설정\n",
        "    df_train = pd.read_csv(root_dir + '/data/train.csv')\n",
        "    df_valid = pd.read_csv(root_dir + '/data/validate.csv')\n",
        "    total_df = pd.concat((df_train.iloc[:-24*7,:], df_valid.iloc[:,:]), axis=0)\n",
        "    print(total_df)\n",
        "\n",
        "    pre_df = preprocess_dataframe(total_df)\n",
        "    mini, maxi = pre_df.iloc[:,CFG['road_number']+2].min(), \\\n",
        "                pre_df.iloc[:,CFG['road_number']+2].max()\n",
        "    print('min, max', mini, maxi)\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(pre_df)\n",
        "    pre_df = scaler.transform(pre_df)\n",
        "\n",
        "    train_data, valid_data = split_data(pre_df, CFG['road_number'])\n",
        "\n",
        "    train_dataset = MyDataset(train_data)\n",
        "    valid_dataset = MyDataset(valid_data)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, \n",
        "        batch_size=CFG['batch_size'],\n",
        "        num_workers=0,\n",
        "        shuffle=True,\n",
        "        pin_memory=use_cuda,\n",
        "        drop_last=False,)\n",
        "\n",
        "    valid_loader = DataLoader(\n",
        "        valid_dataset, \n",
        "        batch_size=CFG['batch_size'],\n",
        "        num_workers=0,\n",
        "        shuffle=False,\n",
        "        pin_memory=use_cuda,\n",
        "        drop_last=False)\n",
        "\n",
        "    # Model, Loss, scheduler, optimizer setting\n",
        "    model = MyModel(input_size=CFG['input_size'],\n",
        "                    hidden_size=CFG['hidden_size'],\n",
        "                    output_size=CFG['output_size'],\n",
        "                    num_layers=CFG['num_layers']).to(device)\n",
        "\n",
        "    init_weight(model, kind=CFG['initialization'])\n",
        "\n",
        "    criterion = MyLoss().to(device)\n",
        "\n",
        "    cosine_annealing_scheduler_arg = dict(\n",
        "        first_cycle_steps=len(train_dataset)//CFG['batch_size'] * CFG['epoch'],\n",
        "        cycle_mult=1.0,\n",
        "        max_lr=1e-04,\n",
        "        min_lr=1e-07,\n",
        "        warmup_steps=len(train_dataset)//CFG['batch_size'] * 3,\n",
        "        gamma=0.9\n",
        "    )\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.000, weight_decay=0)\n",
        "    scheduler = CosineAnnealingWarmupRestarts(optimizer, **cosine_annealing_scheduler_arg)\n",
        "\n",
        "    early_stopper = EarlyStopper(patience=100)\n",
        "    os.makedirs(root_dir + f\"/{CFG['save_dir']}\", exist_ok=True)\n",
        "\n",
        "    print('Start Training!')\n",
        "    for i in range(CFG['epoch']):\n",
        "        print(f\"Epoch :\", i)\n",
        "        train_losses, train_mse = train_one_epoch(model, criterion, train_loader, optimizer, scheduler, device, mini, maxi)\n",
        "        print(\"Train loss, score:\", train_losses / len(train_loader), np.sqrt(train_mse / len(train_loader.dataset)))\n",
        "        valid_losses, valid_mse = valid_one_epoch(model, criterion, valid_loader, device, mini, maxi)\n",
        "        print(\"Valid loss, score:\", valid_losses / len(valid_loader), np.sqrt(valid_mse / len(valid_loader.dataset)))\n",
        "        early_stopper.check_early_stopping(valid_mse / len(valid_loader.dataset))\n",
        "\n",
        "        wandb_dict = {\n",
        "            'train loss': train_losses / len(train_loader),\n",
        "            'train score': np.sqrt(train_mse / len(train_loader.dataset)),\n",
        "            'valid loss': valid_losses / len(valid_loader),\n",
        "            'valid score': np.sqrt(valid_mse / len(valid_loader.dataset)),\n",
        "            'learning rate': scheduler.get_lr()[0]\n",
        "        }\n",
        "\n",
        "        wandb.log(wandb_dict)\n",
        "\n",
        "        print(\"learning rate :\", scheduler.get_lr())\n",
        "\n",
        "        if early_stopper.save_model == True:\n",
        "            dic = {\n",
        "                'model':model.state_dict(),\n",
        "                'optimizer':optimizer.state_dict(),\n",
        "                'scheduler':scheduler.state_dict(),\n",
        "            }\n",
        "            torch.save(dic, root_dir + f\"/{CFG['save_dir']}/{CFG['road_number']:02}_best.pth\")\n",
        "            time.sleep(1)\n",
        "            print('save_model')\n",
        "\n",
        "        if early_stopper.stop:\n",
        "            break\n",
        "\n",
        "    os.rename(\n",
        "        root_dir + f\"/{CFG['save_dir']}/{CFG['road_number']:02d}_best.pth\", \n",
        "        root_dir + f\"/{CFG['save_dir']}/{CFG['road_number']:02d}_{np.sqrt(early_stopper.best_mse):.2f}_{experiment_name}.pth\"\n",
        "        )"
      ],
      "metadata": {
        "id": "EiZvOmtenJ9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(35): # 35 column 학습\n",
        "    setting(i)"
      ],
      "metadata": {
        "id": "8ozpKLYZDBae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "nfb6Uh84J24N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTestDataset(Dataset):\n",
        "    def __init__(self, data, road_number):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.road_number = road_number\n",
        "    \n",
        "    def __len__(self):\n",
        "        return 1\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        src1 = self.data[:7*24, self.road_number]\n",
        "        src2 = self.data[7*24:7*2*24, self.road_number]\n",
        "        trg = self.data[7*2*24:7*3*24, self.road_number]\n",
        "        src_ = np.concatenate((src1[np.newaxis,:], src2[np.newaxis,:]), axis=0)\n",
        "        src = torch.Tensor(src_)\n",
        "        trg = torch.Tensor(trg)\n",
        "        return src, trg"
      ],
      "metadata": {
        "id": "qzNmJ4yyNVBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def infer_setting(number, pths):    \n",
        "    CFG = {\n",
        "        'seed':2022,\n",
        "        'batch_size': 4,\n",
        "        'epoch': 100,\n",
        "        'initialization': \"kaiming\", # kaiming, xavier\n",
        "        'input_size':168,\n",
        "        'hidden_size':1024,\n",
        "        'output_size':168,\n",
        "        'num_layers':2,\n",
        "        'road_number':number\n",
        "    }\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    root_dir = '/content/drive/Othercomputers/내 컴퓨터/workspace/ml_cartraffic'\n",
        "    test_df = pd.read_csv(root_dir + '/data/test.csv')\n",
        "\n",
        "    df_train = pd.read_csv(root_dir + '/data/train.csv')\n",
        "    df_valid = pd.read_csv(root_dir + '/data/validate.csv')\n",
        "    total_df = pd.concat((df_train.iloc[:-24 * 7,:], df_valid), axis=0)\n",
        "    pre_df = preprocess_dataframe(total_df)\n",
        "\n",
        "    test_df_ = pd.concat((df_valid.iloc[-(24*7*2):-24*7,:], test_df.iloc[:]),axis=0)\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(pre_df.iloc[:,2:])\n",
        "    test_df_ = scaler.transform(test_df_.iloc[:,2:])\n",
        "\n",
        "    test_dataset = MyTestDataset(test_df_, CFG['road_number'])\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, \n",
        "        batch_size=1,\n",
        "        num_workers=0,\n",
        "        shuffle=False,\n",
        "        pin_memory=use_cuda,\n",
        "        drop_last=False)\n",
        "    \n",
        "    model = MyModel(input_size=CFG['input_size'],\n",
        "                    hidden_size=CFG['hidden_size'],\n",
        "                    output_size=CFG['output_size'],\n",
        "                    num_layers=CFG['num_layers'])\n",
        "    \n",
        "    checkpoint = torch.load(pths[number])\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    model.to(device)\n",
        "\n",
        "    preds = []\n",
        "\n",
        "    for iter_, sample in enumerate(test_loader):\n",
        "        (src, trg) = sample\n",
        "        (h_in, c_in) = (torch.zeros(model.lstm1.num_layers, src.size(0), model.hidden_size, requires_grad=False).to(device),\n",
        "                        torch.zeros(model.lstm1.num_layers, src.size(0), model.hidden_size, requires_grad=False).to(device))\n",
        "        \n",
        "        src = src.to(device)\n",
        "        pred, (h_in, c_in) = model(src, h_in, c_in)\n",
        "\n",
        "        preds.append(pred)\n",
        "    return preds, scaler"
      ],
      "metadata": {
        "id": "h4R6dD4wMWc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = '/content/drive/Othercomputers/내 컴퓨터/workspace/ml_cartraffic'\n",
        "pths = sorted(glob(root_dir + '/lstm_2week_minmax_since4/*'))\n",
        "\n",
        "submission_table = pd.read_csv(root_dir + '/data/sample_submission.csv')\n",
        "\n",
        "# for i in range(35):\n",
        "#     preds, scaler = infer_setting(i, pths)\n",
        "#     submission_table[submission_table.columns[i+1]] = preds[0][0].detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "eY08lZ1rO5hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMSE 계산"
      ],
      "metadata": {
        "id": "3gmpheWkaLcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summation = 0\n",
        "cnt = 0\n",
        "for i in pths:\n",
        "    if 'best' in i:\n",
        "        float(i.split('/')[-1].split('_')[1])\n",
        "    else:\n",
        "        summation += float(i.split('/')[-1].split('_')[1])\n",
        "    # print(i.split('/')[-1].split('_')[1])\n",
        "    # break\n",
        "summation / 35"
      ],
      "metadata": {
        "id": "0tBLEXWUtM8J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f9d3589-277e-48e9-ebe6-af7c87cc94fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2352.378571428571"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scale_submission = scaler.inverse_transform(submission_table.iloc[:,1:])\n",
        "scale_submission"
      ],
      "metadata": {
        "id": "kKX5GjaR6sQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scale_submission_ = np.where(scale_submission <= 0, 1, scale_submission)"
      ],
      "metadata": {
        "id": "vffrxRTi8rjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_values = pd.DataFrame(scale_submission_, columns=submission_table.columns[1:])"
      ],
      "metadata": {
        "id": "K00POs6WOmrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.concat((submission_table.iloc[:,:1].reset_index(drop=True), \n",
        "           df_values.reset_index(drop=True)), axis=1)"
      ],
      "metadata": {
        "id": "zjjWjs8s9HoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv(root_dir + '/submission_lstm_2week_2layer_since4.csv', index=False)"
      ],
      "metadata": {
        "id": "jQNMt3uQ9XlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP approach"
      ],
      "metadata": {
        "id": "O52xf97uFnuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess"
      ],
      "metadata": {
        "id": "4h9oh5TmXVsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaling(df, y_cols, scaler=None):\n",
        "    if scaler is None:\n",
        "        scaler = StandardScaler()\n",
        "        df[y_cols] = scaler.fit_transform(df[y_cols])\n",
        "        return df, scaler\n",
        "    df[y_cols] = scaler.transform(df[y_cols])\n",
        "    return df\n",
        "\n",
        "def inverse_scaling(df, y_cols, scaler=None):\n",
        "    try: df[y_cols] = scaler.inverse_transform(df[y_cols])\n",
        "    except: df = scaler.inverse_transform(df)\n",
        "    return df\n",
        "\n",
        "def col_to_date(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df.rename(columns={'날짜': 'Date', '시간': 'Hour'})\n",
        "    df['Date'] = pd.to_datetime(df['Date'], format='%Y%m%d')\n",
        "    return df\n",
        "    \n",
        "def preprocess_input_data(df, y_cols, d_hour=168, steps=1, train=True):    \n",
        "    for i in range(d_hour, d_hour+steps):\n",
        "        for y in y_cols:\n",
        "            df[f'{y}D-{i}'] = df[y].shift(i)\n",
        "\n",
        "    x_cols = [i for i in df.columns if i not in y_cols and 'Date' not in i and 'Num' not in i] \n",
        "    if train:\n",
        "        df = df.dropna(axis=0)\n",
        "    return df, x_cols\n",
        "\n",
        "def kf_split(data, fold, n_split, seed, shuffle=True):\n",
        "    if shuffle:\n",
        "        kf = KFold(n_splits=n_split, random_state=seed, shuffle=True)\n",
        "    else:\n",
        "        kf = KFold(n_splits=n_split)\n",
        "        \n",
        "    for i, (train_index, valid_index) in enumerate(kf.split(data)):\n",
        "        if i == fold:\n",
        "            _train, _valid = train_index, valid_index\n",
        "    return _train, _valid"
      ],
      "metadata": {
        "id": "OCloUuX3LFjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_cols = pd.read_csv('./data/train.csv').columns[2:]\n",
        "train_df = col_to_date('./data/train.csv')\n",
        "valid_df = col_to_date('./data/validate.csv')\n",
        "\n",
        "total_df = pd.concat([train_df, valid_df], axis=0)\n",
        "total_df = total_df.drop_duplicates(subset=['Date', 'Hour']).reset_index().drop(['index'], axis=1)\n",
        "\n",
        "# 이상치 조정\n",
        "total_df.loc[total_df['10'] < 100, y_cols] = np.nan\n",
        "total_df = total_df.fillna(method='ffill')\n",
        "total_df[\"DayNum\"] = total_df.Date.dt.weekday\n",
        "\n",
        "# 결측치 추가\n",
        "_feb = {'date': '2020-02-29', 'day_num': 5, 'hours': range(13, 24, 1)}\n",
        "_mar = {'date': '2020-03-30', 'day_num': 0, 'hours': range(2, 24, 1)}\n",
        "\n",
        "group_data = total_df.groupby(['DayNum', 'Hour']).apply(lambda x: np.mean(x))\n",
        "\n",
        "feb, mar = [], []\n",
        "for hour in _feb['hours']:\n",
        "    temp_array = np.array([datetime.strptime(_feb['date'], '%Y-%m-%d')])\n",
        "    temp_array = np.append(temp_array, group_data.loc[_feb['day_num']].loc[hour].values)\n",
        "    feb.append(temp_array)\n",
        "\n",
        "for hour in _mar['hours']:\n",
        "    temp_array = np.array([datetime.strptime(_mar['date'], '%Y-%m-%d')])\n",
        "    temp_array = np.append(temp_array, group_data.loc[_mar['day_num']].loc[hour].values)\n",
        "    mar.append(temp_array)\n",
        "\n",
        "feb_df = pd.DataFrame(feb, columns=total_df.columns)\n",
        "mar_df = pd.DataFrame(mar, columns=total_df.columns)\n",
        "\n",
        "total_df = pd.concat([total_df, feb_df, mar_df], axis=0)\n",
        "total_df['Date'] = pd.to_datetime(total_df['Date'])\n",
        "total_df = total_df.sort_values(['Date', 'Hour'], ascending=[True, True])\n",
        "total_df = total_df.reset_index().drop(['index'], axis=1)\n",
        "\n",
        "test_df = col_to_date('./data/test.csv')\n",
        "test_df = test_df[test_df['10'] == -999].reset_index().drop(['index'], axis=1)\n",
        "\n",
        "# train / test data\n",
        "final_train_df = total_df.copy()\n",
        "final_test_df = test_df.copy()"
      ],
      "metadata": {
        "id": "D5exqX3zFw5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "ByD2o_3AXIJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, x_data, y_data):\n",
        "        self.x_data = x_data\n",
        "        self.y_data = y_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        x = self.x_data[idx]\n",
        "        ys = self.y_data[idx]\n",
        "        return torch.tensor(x).float(), torch.tensor(ys).float()\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, out_features=35):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, 32)\n",
        "        self.fc2 = nn.Linear(32, 32)\n",
        "        self.head = nn.Linear(32, out_features)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.act(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "class RMSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, yhat, y):\n",
        "        return torch.sqrt(self.mse(yhat, y))"
      ],
      "metadata": {
        "id": "PVxcHSG-HOij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(in_f, out_f, lr=3e-4):\n",
        "    model = MLP(in_features=in_f, out_features=out_f)\n",
        "    model = model.to(device)\n",
        "    loss_fn = RMSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    return model, loss_fn, optimizer\n",
        "\n",
        "def train_epoch(model, optimizer, loss_fn, loader):\n",
        "    model.train()\n",
        "    losses = 0.\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = model(x)\n",
        "\n",
        "        loss = loss_fn(outputs, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses += loss\n",
        "    return losses/len(loader)\n",
        "\n",
        "def validate(model, loss_fn, loader, scaler):\n",
        "    model.eval()\n",
        "    losses = scale_losses = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            outputs = model(x)\n",
        "\n",
        "            loss = loss_fn(outputs, y)\n",
        "\n",
        "            scale_outputs = torch.FloatTensor(scaler.inverse_transform(outputs.cpu()))\n",
        "            scale_y = torch.FloatTensor(scaler.inverse_transform(y.cpu()))\n",
        "            scale_loss = loss_fn(scale_outputs, scale_y)\n",
        "\n",
        "            losses += loss\n",
        "            scale_losses += scale_loss\n",
        "    return loss/len(loader), scale_losses/len(loader)\n",
        "\n",
        "def run_epoch(epochs, train_loader, valid_loader, in_f, out_f, y_scaler, init=None, lr=3e-4, wandb=None):\n",
        "    start = time.time()    \n",
        "    model, loss_fn, optimizer = get_model(in_f, out_f, lr=lr)\n",
        "    best_model, best_loss, best_scale, best_epoch = None, float('inf'), float('inf'), 0\n",
        "    \n",
        "    if init is not None:\n",
        "        init_weight(model, kind=init)\n",
        "        \n",
        "    for i in range(epochs):\n",
        "        train_loss = train_epoch(model, optimizer, loss_fn, train_loader)\n",
        "        valid_loss, scale_loss = validate(model, loss_fn, valid_loader, y_scaler)\n",
        "        \n",
        "        if best_loss > valid_loss:\n",
        "            best_model, best_loss = deepcopy(model.state_dict()), valid_loss\n",
        "            \n",
        "        learning_rate = optimizer.param_groups[0]['lr']\n",
        "        if wandb is not None:\n",
        "            wandb_dict = {\n",
        "                'train loss': train_loss,\n",
        "                'valid loss': valid_loss,\n",
        "                'RMSE': scale_loss,\n",
        "            }\n",
        "            wandb.log(wandb_dict)\n",
        "        print(f'epoch {i+1 :2d} train {train_loss :.4f} valid {valid_loss :.4f} {scale_loss :.0f} LR {learning_rate :.5f} TIME {time.time() - start :.2f}s')\n",
        "    return best_model"
      ],
      "metadata": {
        "id": "RTyEmRF1Kaq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = final_train_df.copy()\n",
        "df, y_scaler = scaling(df, y_cols)\n",
        "df, x_cols = preprocess_input_data(df, y_cols, d_hour=168, steps=1)\n",
        "\n",
        "x_data = df[x_cols].values\n",
        "y_data = df[y_cols].values\n",
        "\n",
        "train_index, valid_index = kf_split(x_data, 1, 5, seed=2022)\n",
        "x_train, y_train = x_data[train_index], y_data[train_index]\n",
        "x_valid, y_valid = x_data[valid_index], y_data[valid_index]\n",
        "\n",
        "train_set = CustomDataset(x_train, y_train)\n",
        "valid_set = CustomDataset(x_valid, y_valid)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=1, pin_memory=True, shuffle=True, drop_last=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=1, pin_memory=True)"
      ],
      "metadata": {
        "id": "W4zOaW9lPRcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "best_model = run_epoch(epochs, train_loader, valid_loader, in_f=len(x_cols), out_f=len(y_cols), y_scaler=y_scaler)"
      ],
      "metadata": {
        "id": "myE1eBg5RU9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "wUs4m8CFXMdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(state_dict: dict, x_test: np.array, x_cols):\n",
        "    model = MLP(in_features=len(x_cols), out_features=35)\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    logits = model(torch.FloatTensor(x_test).to(device))\n",
        "    return logits"
      ],
      "metadata": {
        "id": "B5hUYUiIO2om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = pd.Series([9999] * 3480)\n",
        "_train = final_train_df.copy().set_index(s)\n",
        "_train = scaling(_train, y_cols, y_scaler)\n",
        "\n",
        "_test = final_test_df.copy()\n",
        "_test = pd.concat([_train, _test], axis=0)\n",
        "_test, _ = preprocess_input_data(_test, y_cols, train=False)\n",
        "\n",
        "in_test = _test.copy()\n",
        "for v in range(168):\n",
        "    test_pred = inference(best_model, in_test.loc[v][x_cols].values.astype('float64').reshape(1, -1), x_cols)\n",
        "    in_test.loc[v, y_cols] = test_pred.squeeze().detach().cpu().numpy()\n",
        "\n",
        "out_test = in_test.copy()\n",
        "out_test = inverse_scaling(out_test, y_cols, y_scaler)\n",
        "result = out_test[y_cols].loc[0:167]\n",
        "\n",
        "submission = pd.read_csv('./data/sample_submission.csv')\n",
        "for i in range(168):\n",
        "    submission.loc[i, y_cols] = result.loc[i].values\n",
        "\n",
        "submission.to_csv('./_submission.csv', index=False)"
      ],
      "metadata": {
        "id": "E2mlzY5-OoAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble"
      ],
      "metadata": {
        "id": "IpN04oOGaO53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv('/content/drive/Othercomputers/내 컴퓨터/workspace/ml_cartraffic/submission_lstm_2week_2layer_since4.csv')\n",
        "df2 = pd.read_csv('/content/drive/Othercomputers/내 컴퓨터/workspace/ml_cartraffic/7_2760.1_submission.csv')\n",
        "timestamp = df1.iloc[:,0]\n",
        "value = (df1.iloc[:,1:] + df2.iloc[:,1:]) * 0.5\n",
        "pd.concat((timestamp, value), axis=1).to_csv('/content/drive/Othercomputers/내 컴퓨터/workspace/ml_cartraffic/ensemble.csv', index=False)"
      ],
      "metadata": {
        "id": "7zy0Uxj_IPI8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "traffic_codes",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}